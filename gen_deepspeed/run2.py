# based on https://github.com/microsoft/DeepSpeedExamples/blob/master/inference/huggingface/text-generation/inference-test.py

import json
import time
import pickle
import torch
import deepspeed
import transformers

from arguments import parser


PROMPT_COT = "\n".join([
        "Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. "
        "How many trees did the grove workers plant today?",
        "Let's think step by step: There are 15 trees originally. Then there were 21 trees after some more were planted. "
        "So there must have been 21 - 15 = 6. The answer is 6.",
        "Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?",
        "Let's think step by step: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.",
        "Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?",
        "Let's think step by step: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. "
        "After eating 35, they had 74 - 35 = 39. The answer is 39.",
        "Question: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?",
        "Let's think step by step: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. "
        "So he gave Denny 20 - 12 = 8. The answer is 8.",
        "Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?",
        "Let's think step by step: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. "
        "5 + 4 = 9. The answer is 9.",
        "Question: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. "
        "How many computers are now in the server room?",
        "Let's think step by step: There were originally 9 computers. For each of 4 days, 5 more computers were added. "
        "So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.",
        "Question: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. "
        "How many golf balls did he have at the end of wednesday?",
        "Let's think step by step: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. "
        "After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.",
        "Question: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?",
        "Let's think step by step: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. "
        "So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.",
    ])


class CustomDSPipeline:
    """inference pipeline based on DSPipeline"""

    def __init__(
        self,
        model_name,
        dtype=torch.float16,
        device=-1,
    ):
        self.model_name = model_name
        self.dtype = dtype

        if isinstance(device, torch.device):
            self.device = device
        elif isinstance(device, str):
            self.device = torch.device(device)
        elif device < 0:
            self.device = torch.device("cpu")
        else:
            self.device = torch.device(f"cuda:{device}")

        self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_name)
        self.model.eval()

        if self.dtype == torch.float16:
            self.model.half()

    def generate_outputs(self, input_ids, **kwargs):
        """
        Generate outputs based on the given input IDs.
        Args:
            input_ids (torch.Tensor): Tensor of input IDs.
            **kwargs: Additional keyword arguments for the `generate` method of the model.

        Returns:
            torch.Tensor: Outputs generated by the model.
        """
        input_ids = input_ids.to(self.device)
        self.model.cuda().to(self.device)

        outputs = self.model.generate(
            inputs=input_ids, generation_config=None, **kwargs
        )
        return outputs


def get_dataset(pickle_path=None):
    """obtain gsm8k-based dataset"""
    if pickle_path is not None:
        with open(pickle_path, "rb") as f:
            input_sentences = pickle.load(f)
    # else:
    #     train_examples = get_examples(
    #         "train", data_path=os.path.join(GSM8K_PATH, "grade_school_math/data")
    #     )
    #     input_sentences = [te["question"] for te in train_examples]

    print(f"{len(input_sentences)=}")
    return input_sentences


def main(args):
    """just main"""
    if args.hf_baseline and args.world_size > 1:
        raise RuntimeError("Only `--num_gpus 1` supported for non-DeepSpeed uses")

    data_type = getattr(torch, args.dtype)

    if args.local_rank == 0:
        # see_memory_usage("before init", True)
        t0 = time.time()

    pipe = CustomDSPipeline(
        model_name=args.model,
        dtype=data_type,
        #   is_meta=False,  # args.use_meta_tensor,
        device=args.local_rank,
        #   checkpoint_path=args.checkpoint_path
    )

    if args.local_rank == 0:
        print(f"initialization time: {(time.time()-t0):.3f} s")
        # see_memory_usage("after init", True)

    pipe.model = deepspeed.init_inference(
        pipe.model,
        dtype=data_type,
        mp_size=args.world_size,
        replace_with_kernel_inject=args.use_kernel,
        max_tokens=args.max_tokens,
        save_mp_checkpoint_path=args.save_mp_checkpoint_path,
    )

    dataset = get_dataset("input_sentences.pkl")
    # prompt_cot = get_cot_hub_prompt(raw=False)
    prefix_tokenizer = transformers.AutoTokenizer.from_pretrained(args.model)
    prefix_tokenizer.padding_side = "left"
    prefix_tokenizer.pad_token = prefix_tokenizer.eos_token

    results = []
    times = []

    batch_size = args.batch_size
    for i in range(args.start_id, min(args.end_id, len(dataset)), batch_size):
        start = time.time()
        data_ids = [i + j for j in range(batch_size)]
        batch0 = [dataset[j] for j in data_ids]
        batch1 = [
            f"{PROMPT_COT}\nQuestion: {q}Let's think step by step:" for q in batch0
        ]
        batch_pt = prefix_tokenizer(batch1, return_tensors="pt", padding="longest")

        if args.local_rank == 0:
            print(f"batch # {i:>3}  ", end="")

        q_length = batch_pt["input_ids"].shape[-1]
        torch.cuda.synchronize()

        output_tokens = pipe.generate_outputs(
            batch_pt["input_ids"],
            max_new_tokens=args.max_new_tokens,
            do_sample=(not args.greedy),
            num_return_sequences=8,
            top_k=50,
            top_p=0.95,
            use_cache=True,
            pad_token_id=2,  # prefix_tokenizer.eos_token_ids
        )
        output_tokens = output_tokens[:, q_length:]  # stripping the prompt and question
        outputs = prefix_tokenizer.batch_decode(output_tokens, skip_special_tokens=True)

        torch.cuda.synchronize()
        end = time.time()
        times.append(end - start)
        if args.local_rank == 0:
            print(f"time: {end - start:.3f} s.")
            res = dict(
                i=i,
                data_ids=data_ids,
                rank=args.local_rank,
                time=-1 if args.local_rank != 0 else end - start,
                inputs=batch0,
                outputs=outputs,
                # output_tokens=output_tokens,
                # batch_pt=batch_pt
            )
            results.append(res)

        # break

    # print("PERF STATS:")
    # Performance.print_perf_stats(
    #     map(lambda t: t / args.max_new_tokens, times),
    #     pipe.model.config,
    #     args.dtype,
    #     args.batch_size,
    # )

    results_j = json.dumps(results)
    with open(args.out_file, "w", encoding='utf-8') as outfile:
        outfile.write(results_j)
    # torch.save(results, 'results.pt')

    print("prefix_tokenizer.eos_token_id", prefix_tokenizer.eos_token_id)



if __name__ == "__main__":
    args_ = parser.parse_args()
    main(args_)
